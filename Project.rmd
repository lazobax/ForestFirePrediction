---
title: "Project"
author: "Lazo Ali & Lia Shahine"
date: "2024-12-06"
output: html_document
---

```{r include=FALSE}
set.seed(6122024)
```

# Introduction

# Exploratory Data Analysis

The CSV file provided for this project contained two distinct sections. One for Cordillera and another for Hudson Bay. The data were unified in excel and a column was added indicating the city for each row.Let's start exploring the data!

First we load the data and ensure its loaded properly:

```{r}
rawData <- read.csv("ff_data.csv")
head(rawData)
```

The data appears to be loaded properly, let's see what our data looks like.

the columns are:

```{r}
colnames(rawData)
```

the dimensions of our data set:

```{r}
dim(rawData)
```

We have 15 columns and 244 observations.

Now let's look at the data types:

```{r}
str(rawData)
```

It appears as though `DC` and `FWI`, which should be of type `num`, are interpreted as type `chr`, let's explore why this is by returning the rows with non-numeric `FWI` variable:

```{r echo=FALSE}
non_numeric_rows <- rawData[!grepl("^\\d*\\.?\\d+$", rawData$FWI), ]
non_numeric_rows
```

If we observe the `DC` variable for this row, it indicates that there must have been a (un/mis)placed comma in the original `forest_fires_dataset.csv`. Upon manual inspection, this does seem to be true.

```{bash}
grep "14,7,2012" forest_fires_dataset.csv
```

The first observation is that of Cordillera and the second that of Hudson Bay. As we see, the data is not formatted properly. We can assume that the correct formatting of this row should be:

```         
14,7,2012,37,37,18,0.2,88.9,12.9,14.6,9,12.5,10.4,fire,Hudson Bay
```

However, this assumption could negatively impact downstream analysis if not dealt with correctly. We can't simply assume that the value `9` should be put in the ISI section without further analysis of this assumption. For now, we will remove this row from the dataset:

```{r}
rawData <- rawData[setdiff(rownames(rawData), rownames(non_numeric_rows)), ]
correctedRow <- c(14,7,2012,37,37,18,0.2,88.9,12.9,14.6,9,12.5,10.4,"fire","Hudson Bay")
```

We will now convert the columns to numeric:

```{r}
columnsToConvert <- c("DC","FWI")
rawData[columnsToConvert] <- lapply(rawData[columnsToConvert], as.numeric)
```

Let's check the data types again:

```{r}
str(rawData[columnsToConvert])
```

It's correct!

let's see if the classes and cities are imported with issues:

```{r}
unique(rawData$Class)
```

indeed, let's fix this

```{r}
# Replace rows where 'Class' contains "fire" but does not contain "not" with "F"
rawData$Class[grep("fire", rawData$Class) & !grepl("not", rawData$Class)] <- "F"

# Use grep to find rows where Class contains "not" and change to "NF"
rawData$Class[grep("not", rawData$Class)] <- "NF"
```

now let's check the same with the City variable

```{r}
unique(rawData$City)
```

all good!

now, let's check for missing values:

```{r}
colSums(is.na(rawData))
```

There are none!

now let's define a function to plot a box and whisker plot and descriptive statistics given a dataframe and a column name:

```{r}
plot_with_stats <- function(column_name, dataframe) {
  # Extract the column
  column_data <- dataframe[[column_name]]
  
  # Calculate descriptive statistics
  stats <- list(
    Min = min(column_data, na.rm = TRUE),
    Q1 = quantile(column_data, 0.25, na.rm = TRUE),
    Median = median(column_data, na.rm = TRUE),
    Q3 = quantile(column_data, 0.75, na.rm = TRUE),
    Max = max(column_data, na.rm = TRUE),
    Mean = mean(column_data, na.rm = TRUE),
    SD = sd(column_data, na.rm = TRUE)
  )
  
  # Adjust plot size to accommodate text
  par(mar = c(5, 4, 4, 10))  # Increase right margin for text
  
  # Create the boxplot
  boxplot(column_data, 
          main = paste("Boxplot of", column_name), 
          col = "skyblue",
          xlab = column_name)
  
  # Prepare the text for annotation
  stats_text <- paste(
    "Min:", round(stats$Min, 2), "\n",
    "Q1:", round(stats$Q1, 2), "\n",
    "Median:", round(stats$Median, 2), "\n",
    "Q3:", round(stats$Q3, 2), "\n",
    "Max:", round(stats$Max, 2), "\n",
    "Mean:", round(stats$Mean, 2), "\n",
    "SD:", round(stats$SD, 2)
  )
  
  # Add the text outside the plot area
  mtext(stats_text, side = 4, line = 5, adj = 0, cex = 0.8, las = 1)}


```

Let's get all our numerical columns in one vector:

```{r}
numerical_columns <- names(rawData)[sapply(rawData, is.numeric)]
for (column in numerical_columns[!numerical_columns %in% c("Day", "Month", "Year")]) {
  plot_with_stats(column, rawData)
}

```

```{r}
library(ggplot2)

# Create a copy of rawData for plotting without modifying rawData
rawDataForPlot <- rawData

# Select numerical columns from rawDataForPlot
numerical_columns <- names(rawDataForPlot)[sapply(rawDataForPlot, is.numeric)]
numerical_columns <- numerical_columns[!numerical_columns %in% c("Day", "Month", "Year")]

numerical_data <- rawDataForPlot[, numerical_columns]

# Define colors and shapes for the existing categories in rawDataForPlot
colors <- c("F" = "#d95f02", "NF" = "#1b9e77")
shapes <- c("Hudson Bay" = 21, "Cordillera" = 24)

# Loop through each pair of numerical columns and create scatter plots
for (i in 1:(length(numerical_columns) - 1)) {
  for (j in (i + 1):length(numerical_columns)) {
    
    x_var <- numerical_columns[i]
    y_var <- numerical_columns[j]
    
    plot <- ggplot(rawDataForPlot, aes_string(x = x_var, y = y_var)) +
      geom_point(aes(fill = Class, shape = City), size = 3, stroke = 1) +  # Map fill and shape
      scale_fill_manual(values =  c("F" = "#d95f02", "NF" = "#1b9e77"), name = "Class", ) +  # Apply colors to Class
      scale_shape_manual(values = shapes, name = "City") +  # Apply shapes to City
      labs(
        title = paste("Scatterplot of", x_var, "vs", y_var),
        x = x_var,
        y = y_var
      ) +
      theme_minimal() +
      theme(
        legend.position = "top",
        legend.direction = "horizontal",
        legend.title = element_text(face = "bold"),
        legend.text = element_text(size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5)
      )
    
    print(plot)
  }
}


```

That shit above didn't work we will come back to it later, for now we continue

Class imbalance:

```{r}

library(ggplot2)

# Create a new data frame for plotting without modifying rawData
rawDataForPlot <- rawData

# Ensure Class is a factor with proper levels in the new data frame
rawDataForPlot$Class <- factor(rawDataForPlot$Class, levels = c("F", "NF"))

# Create a new data frame for the counts of each class
class_counts <- as.data.frame(table(rawDataForPlot$Class))
colnames(class_counts) <- c("Class", "Count")

# Create a bar plot of the Class variable with specified colors and count labels
ggplot(rawDataForPlot, aes(x = Class)) +
  geom_bar(aes(fill = Class), color = "black") +  # Fill bars based on Class
  scale_fill_manual(values = c("F" = "#d95f02", "NF" = "#1b9e77")) +  # Custom colors for F and NF
  labs(
    title = "Distribution of Classes",
    x = "Class",
    y = "Count"
  ) +
  geom_text(data = class_counts, aes(x = Class, y = Count, label = Count), vjust = -0.5) +  # Add count labels above bars
  theme_minimal() +
  theme(
    legend.position = "none",  # Remove the legend (optional)
    plot.title = element_text(face = "bold", hjust = 0.5)  # Center and bold title
  )


```
Classes don't look imbalanced 



# Data Pre-Processing
we will do a generic pre-processing here, all but normalizing the numeric values, this is because different ML models will benefit from different normalization methods:

```{r}
# Create a new data frame for preprocessing to avoid modifying rawData
rawDataForML <- rawData

# Step 1: Drop the 'Year' column
rawDataForML$Year <- NULL

# Step 2: Encode 'Class' and 'City'
# Encode 'Class' as -1 for 'F' and 1 for 'NF'
rawDataForML$Class <- ifelse(rawDataForML$Class == "F", -1, 1)

# Encode 'City' as -1 for 'Cordillera' and 1 for 'Hudson Bay'
rawDataForML$City <- ifelse(rawDataForML$City == "Cordillera", -1, 1)

# Show the first few rows of the preprocessed data
head(rawDataForML)

```



we will use the distributions to guide the normalization process for each model 
```{r}
library(ggplot2)

# Select numeric columns
numerical_columns <- names(rawDataForML)[sapply(rawDataForML, is.numeric)]

# Loop through each numeric variable and plot the distribution
for (col in numerical_columns) {
  # Create a histogram for each numeric column
  plot <- ggplot(rawDataForML, aes_string(x = col)) +
    geom_histogram(bins = 30, fill = "#d95f02", color = "black", alpha = 0.7) +
    labs(
      title = paste("Distribution of", col),
      x = col,
      y = "Frequency"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", hjust = 0.5)
    )
  
  # Print the plot
  print(plot)
}

```

# Model Development


## Logistic Regression



In this section, we will build a **Logistic Regression** model to predict whether a fire will occur based on environmental factors. The outcome we are predicting is categorical, either **"fire"** or **"no fire"**. 

### Why Logistic Regression?

Logistic regression is used for classification tasks, making it suitable for predicting categorical outcomes like "fire" or "no fire". Since the goal is to classify whether a fire is likely to occur, logistic regression is an appropriate choice. 

### Model Evaluation

We will assess the performance of our model using the **ROC curve** (Receiver Operating Characteristic curve) and the **AUC** (Area Under Curve). The ROC curve compares the true positive rate (sensitivity) with the false positive rate (1-specificity). A **higher ROC curve** indicates better performance, and a **higher AUC (closer to 1)** indicates a better model fit.

### Steps for Building the Model

We will first build the logistic regression model without normalizing the data and then compare the results after applying **Z-score normalization**. Normalization is essential in logistic regression because it standardizes features that have different units or scales. Without normalization, features with larger ranges can dominate the model and impact its accuracy.

### 1. Build Model Without Normalization

```r
# Split the data into training and testing sets (80% train, 20% test)
set.seed(123)
trainIndex <- createDataPartition(rawData$Classes, p = 0.8, list = FALSE)
trainData <- rawData[trainIndex, ]
testData <- rawData[-trainIndex, ]

# Fit the Logistic Regression model
logit_model <- glm(Classes ~ Temp + RH + Ws + Rain + FFMC + DMC + DC + ISI + BUI + FWI, 
                   data = trainData, 
                   family = binomial)

# Summary of the model
summary(logit_model)

# Predict on the test data
predictions <- predict(logit_model, testData, type = "response")
predicted_classes <- ifelse(predictions > 0.5, "fire", "not fire")

# Confusion Matrix
conf_matrix <- confusionMatrix(factor(predicted_classes), testData$Classes)
print(conf_matrix)

# Calculate ROC curve and AUC
roc_curve <- roc(testData$Classes, predictions)
plot(roc_curve, main = "ROC Curve - Logistic Regression")
auc(roc_curve)


## Linear Discriminant Analysis

## Quadratic Discriminant Analysis

## K-Nearest Neighbors Classifier

## Tree Based Methods

## Support Vector Machine

# ## Support Vector Machine (SVM) Model

In this section, we will build a Support Vector Machine (SVM) model to predict if a fire will occur   based on environmental factors (independent variables). SVM works by finding the hyperplane that best  separates the classes in the feature space. We will first train the SVM model using the raw data (without normalization) 
and evaluate it using the confusion matrix, ROC curve, and AUC. Afterward, we will normalize the data using Z-score normalization 
to see if it improves the model’s performance.

# Load necessary libraries
library(e1071)  # For SVM
library(caret)  # For data partitioning and evaluation
library(ggplot2)  # For visualization
library(patchwork)  # For combining plots

# Preprocessing: Create a new data frame for preprocessing to avoid modifying rawData
rawDataForML <- rawData

# Step 1: Drop the 'Year' column (assuming it's not needed)
rawDataForML$Year <- NULL

# Step 2: Encode 'Class' and 'City'
# Encode 'Class' as -1 for 'F' and 1 for 'NF'
rawDataForML$Class <- ifelse(rawDataForML$Class == "F", -1, 1)

# Encode 'City' as -1 for 'Cordillera' and 1 for 'Hudson Bay'
rawDataForML$City <- ifelse(rawDataForML$City == "Cordillera", -1, 1)

# Function to run SVM with different kernels and cost values
run_svm <- function(data, exclude_cols, kernel = "linear", cost = 1, gamma = NULL, degree = 3, scale = TRUE) {
  set.seed(6122024)  # Set seed for reproducibility
  
  # Exclude specified columns
  data <- data[, !names(data) %in% exclude_cols]
  
  # Ensure Class is a factor
  data$Class <- as.factor(data$Class)
  
 
  # Split data into training and test sets (75:25 split)
  train_index <- createDataPartition(data$Class, p = 0.75, list = FALSE)
  train_data <- data[train_index, ]
  test_data <- data[-train_index, ]
  
  # Train the SVM model
  svm_model <- svm(
    Class ~ ., 
    data = train_data, 
    kernel = kernel, 
    cost = cost, 
    gamma = gamma, 
    degree = degree, 
    scale = scale
  )
  
  # Make predictions on the test set
  test_predictions <- predict(svm_model, newdata = test_data)
  
  # Evaluate performance
  conf_matrix <- confusionMatrix(test_predictions, test_data$Class)
  
  # Extract confusion matrix as a dataframe
  conf_matrix_table <- as.data.frame(conf_matrix$table)
  names(conf_matrix_table) <- c("Reference", "Prediction", "Freq")
  
  # Plot confusion matrix
  conf_matrix_plot <- ggplot(conf_matrix_table, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white") +
    geom_text(aes(label = Freq), vjust = 1) +
    scale_fill_gradient(low = "white", high = "blue") +
    theme_minimal() +
    labs(title = paste("Confusion Matrix (Kernel:", kernel, ")"), x = "Actual", y = "Predicted")
  
  # Calculate metrics
  accuracy <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
  precision <- conf_matrix$byClass["Pos Pred Value"]
  recall <- conf_matrix$byClass["Sensitivity"]
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  # Create a dataframe for descriptive statistics
  metrics_df <- data.frame(
    Metric = c("Accuracy", "Precision", "Recall", "F1 Score"),
    Value = c(accuracy, precision, recall, f1_score)
  )
  
  # Plot descriptive statistics
  metrics_plot <- ggplot(metrics_df, aes(x = Metric, y = Value, fill = Metric)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    geom_text(aes(label = round(Value, 2)), vjust = -0.5) +
    theme_minimal() +
    labs(title = "Performance Metrics", y = "Value", x = "") +
    ylim(0, 1)
  
  # Combine the plots and display
  combined_plot <- conf_matrix_plot + metrics_plot
  print(combined_plot)
  
  # Return performance metrics for reporting
  return(list(
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    F1_Score = f1_score,
    Confusion_Matrix = conf_matrix,
    Model = svm_model  # Return the model for further use
  ))
}


# Testing different kernels with default cost=1, gamma=0.1, degree=3
results_poly <- run_svm(rawDataForML, exclude_cols = c("Day", "Month", "Year"), kernel = "polynomial", cost = 1, gamma = 0.1, degree = 3)
results_rbf <- run_svm(rawDataForML, exclude_cols = c("Day", "Month", "Year"), kernel = "radial", cost = 1, gamma = 0.1)
results_sigmoid <- run_svm(rawDataForML, exclude_cols = c("Day", "Month", "Year"), kernel = "sigmoid", cost = 1, gamma = 0.1)

# Now, let's test with different cost values to observe the impact on support vectors and performance

# Test with higher cost values
results_poly1 <- run_svm(rawDataForML, exclude_cols = c("Day", "Month", "Year"), kernel = "polynomial", cost = 10, gamma = 0.1, degree = 3)
results_rbf1 <- run_svm(rawDataForML, exclude_cols = c("Day", "Month", "Year"), kernel = "radial", cost = 0.01, gamma = 0.1)
results_sigmoid1 <- run_svm(rawDataForML, exclude_cols = c("Day", "Month", "Year"), kernel = "sigmoid", cost = 100, gamma = 0.1)

# Observations and explanations:
# - As cost increases, we expect the model to prioritize reducing misclassifications, which may lead to more support vectors.
# - When cost is smaller, the margin becomes wider, and there may be fewer support vectors, allowing for more misclassification.
# The choice of kernel will impact how well the model can fit the data, and different kernels may have varying performance across the different datasets.

# Print results for different kernels
print(results_poly)
print(results_rbf)
print(results_sigmoid)

# Print results with different cost values
print(results_poly1)
print(results_rbf1)
print(results_sigmoid1)
```
Analsyis of the output:

The SVM model's performance varied based on the kernel and cost value. The RBF kernel with cost 1 yielded the highest accuracy (98.33%) and fewer support vectors (74). The polynomial and sigmoid kernels performed similarly with an accuracy of 96.67%, though the sigmoid kernel had fewer support vectors (56). When cost increased (e.g., cost = 10), accuracy dropped slightly, especially with the polynomial kernel (95%) and the RBF kernel with low cost (0.01) resulted in overfitting (56.67%). Overall, the RBF kernel with cost 1 proved to be the most effective configuration.

Cost and Support Vectors: As the cost increases (e.g., from 1 to 10), the number of support vectors generally decreases. A higher cost places more emphasis on correctly classifying the training data, leading to fewer support vectors and a more complex model.
Model Performance: The RBF kernel generally gives the best performance in terms of accuracy, while the polynomial kernel performs similarly to the sigmoid kernel but with a slightly larger number of support vectors.
Impact of Low Cost: With lower cost (e.g., cost = 0.01), the model becomes more tolerant of misclassification, leading to wider margins but more errors in prediction, as seen with the RBF kernel's significant drop in accuracy.
These results indicate the sensitivity of the SVM to cost and kernel choice, highlighting the importance of tuning these parameters for optimal performance.

#Now we test the SVM on normalized data , using the Min-Max normilazation

```{r}
preProc <- preProcess(rawDataForML[, !names(rawDataForML) %in% c("Class", "City")], method = "range")
normalized_data <- predict(preProc, rawDataForML)

# Testing different kernels with normalized data and default cost=1, gamma=0.1, degree=3
results_poly <- run_svm(normalized_data, exclude_cols = c("Day", "Month", "Year"), kernel = "polynomial", cost = 1, gamma = 0.1, degree = 3)
results_rbf <- run_svm(normalized_data, exclude_cols = c("Day", "Month", "Year"), kernel = "radial", cost = 1, gamma = 0.1)
results_sigmoid <- run_svm(normalized_data, exclude_cols = c("Day", "Month", "Year"), kernel = "sigmoid", cost = 1, gamma = 0.1)

# Now, let's test with different cost values to observe the impact on support vectors and performance

# Test with higher cost values
results_poly1 <- run_svm(normalized_data, exclude_cols = c("Day", "Month", "Year"), kernel = "polynomial", cost = 10, gamma = 0.1, degree = 3)
results_rbf1 <- run_svm(normalized_data, exclude_cols = c("Day", "Month", "Year"), kernel = "radial", cost = 0.01, gamma = 0.1)
results_sigmoid1 <- run_svm(normalized_data, exclude_cols = c("Day", "Month", "Year"), kernel = "sigmoid", cost = 100, gamma = 0.1)

# Observations and explanations:
# - As cost increases, we expect the model to prioritize reducing misclassifications, which may lead to more support vectors.
# - When cost is smaller, the margin becomes wider, and there may be fewer support vectors, allowing for more misclassification.
# The choice of kernel will impact how well the model can fit the data, and different kernels may have varying performance across the different datasets.

# Print results for different kernels
print(results_poly)
print(results_rbf)
print(results_sigmoid)

# Print results with different cost values
print(results_poly1)
print(results_rbf1)
print(results_sigmoid1)
```
The analysis of SVM performance with normalized (testing with the same changes of cost and kernel) data tells us the following:

The RBF kernel with a cost of 1 gives the best results, achieving 98.33% accuracy, with high precision (1.0) and sensitivity (0.9706). The number of support vectors is 74.
The polynomial kernel with cost 1 shows good performance with 96.67% accuracy, 1.0 sensitivity, and 0.944 precision. It uses 90 support vectors.
The sigmoid kernel also performs well with 96.67% accuracy, 1.0 specificity, and 0.9412 recall, but with fewer support vectors (56).
With higher cost values:

The polynomial kernel with cost 10 sees a slight drop in accuracy (95%) but still maintains solid performance.
The RBF kernel with cost 0.01 drops drastically to 56.67% accuracy, overfitting the data with excessive support vectors (161).
The sigmoid kernel with cost 100 gives 91.67% accuracy, showing a balanced performance with a decent precision of 0.9394.
In conclusion, the RBF kernel is the most effective for this dataset, while cost adjustments impact the model’s ability to generalize.

## Analysis

# Results and Interpretation

# Conclusion
