---
title: "Project"
author: "Lazo Ali & Lia Shahine"
date: "2024-12-06"
output: html_document
---

```{r include=FALSE}
set.seed(6122024)
```

# Introduction

# Exploratory Data Analysis

The CSV file provided for this project contained two distinct sections. One for Cordillera and another for Hudson Bay. The data were unified in excel and a column was added indicating the city for each row.Let's start exploring the data!

First we load the data and ensure its loaded properly:

```{r}
rawData <- read.csv("ff_data.csv")
head(rawData)
```

The data appears to be loaded properly, let's see what our data looks like.

the columns are:

```{r}
colnames(rawData)
```

the dimensions of our data set:

```{r}
dim(rawData)
```

We have 15 columns and 244 observations.

Now let's look at the data types:

```{r}
str(rawData)
```

It appears as though `DC` and `FWI`, which should be of type `num`, are interpreted as type `chr`, let's explore why this is by returning the rows with non-numeric `FWI` variable:

```{r echo=FALSE}
non_numeric_rows <- rawData[!grepl("^\\d*\\.?\\d+$", rawData$FWI), ]
non_numeric_rows
```

If we observe the `DC` variable for this row, it indicates that there must have been a (un/mis)placed comma in the original `forest_fires_dataset.csv`. Upon manual inspection, this does seem to be true.

```{bash}
grep "14,7,2012" forest_fires_dataset.csv
```

The first observation is that of Cordillera and the second that of Hudson Bay. As we see, the data is not formatted properly. We can assume that the correct formatting of this row should be:

```         
14,7,2012,37,37,18,0.2,88.9,12.9,14.6,9,12.5,10.4,fire,Hudson Bay
```

However, this assumption could negatively impact downstream analysis if not dealt with correctly. We can't simply assume that the value `9` should be put in the ISI section without further analysis of this assumption. For now, we will remove this row from the dataset:

```{r}
rawData <- rawData[setdiff(rownames(rawData), rownames(non_numeric_rows)), ]
correctedRow <- c(14,7,2012,37,37,18,0.2,88.9,12.9,14.6,9,12.5,10.4,"fire","Hudson Bay")
```

We will now convert the columns to numeric:

```{r}
columnsToConvert <- c("DC","FWI")
rawData[columnsToConvert] <- lapply(rawData[columnsToConvert], as.numeric)
```

Let's check the data types again:

```{r}
str(rawData[columnsToConvert])
```

It's correct!

let's see if the classes and cities are imported with issues:

```{r}
unique(rawData$Class)
```

indeed, let's fix this

```{r}
# Replace rows where 'Class' contains "fire" but does not contain "not" with "F"
rawData$Class[grep("fire", rawData$Class) & !grepl("not", rawData$Class)] <- "F"

# Use grep to find rows where Class contains "not" and change to "NF"
rawData$Class[grep("not", rawData$Class)] <- "NF"
```

now let's check the same with the City variable

```{r}
unique(rawData$City)
```

all good!

now, let's check for missing values:

```{r}
colSums(is.na(rawData))
```

There are none!

now let's define a function to plot a box and whisker plot and descriptive statistics given a dataframe and a column name:

```{r}
plot_with_stats <- function(column_name, dataframe) {
  # Extract the column
  column_data <- dataframe[[column_name]]
  
  # Calculate descriptive statistics
  stats <- list(
    Min = min(column_data, na.rm = TRUE),
    Q1 = quantile(column_data, 0.25, na.rm = TRUE),
    Median = median(column_data, na.rm = TRUE),
    Q3 = quantile(column_data, 0.75, na.rm = TRUE),
    Max = max(column_data, na.rm = TRUE),
    Mean = mean(column_data, na.rm = TRUE),
    SD = sd(column_data, na.rm = TRUE)
  )
  
  # Adjust plot size to accommodate text
  par(mar = c(5, 4, 4, 10))  # Increase right margin for text
  
  # Create the boxplot
  boxplot(column_data, 
          main = paste("Boxplot of", column_name), 
          col = "skyblue",
          xlab = column_name)
  
  # Prepare the text for annotation
  stats_text <- paste(
    "Min:", round(stats$Min, 2), "\n",
    "Q1:", round(stats$Q1, 2), "\n",
    "Median:", round(stats$Median, 2), "\n",
    "Q3:", round(stats$Q3, 2), "\n",
    "Max:", round(stats$Max, 2), "\n",
    "Mean:", round(stats$Mean, 2), "\n",
    "SD:", round(stats$SD, 2)
  )
  
  # Add the text outside the plot area
  mtext(stats_text, side = 4, line = 5, adj = 0, cex = 0.8, las = 1)}


```

Let's get all our numerical columns in one vector:

```{r}
numerical_columns <- names(rawData)[sapply(rawData, is.numeric)]
for (column in numerical_columns[!numerical_columns %in% c("Day", "Month", "Year")]) {
  plot_with_stats(column, rawData)
}

```

```{r}
library(ggplot2)

# Create a copy of rawData for plotting without modifying rawData
rawDataForPlot <- rawData

# Select numerical columns from rawDataForPlot
numerical_columns <- names(rawDataForPlot)[sapply(rawDataForPlot, is.numeric)]
numerical_columns <- numerical_columns[!numerical_columns %in% c("Day", "Month", "Year")]

numerical_data <- rawDataForPlot[, numerical_columns]

# Define colors and shapes for the existing categories in rawDataForPlot
colors <- c("F" = "#d95f02", "NF" = "#1b9e77")
shapes <- c("Hudson Bay" = 21, "Cordillera" = 24)

# Loop through each pair of numerical columns and create scatter plots
for (i in 1:(length(numerical_columns) - 1)) {
  for (j in (i + 1):length(numerical_columns)) {
    
    x_var <- numerical_columns[i]
    y_var <- numerical_columns[j]
    
    plot <- ggplot(rawDataForPlot, aes_string(x = x_var, y = y_var)) +
      geom_point(aes(fill = Class, shape = City), size = 3, stroke = 1) +  # Map fill and shape
      scale_fill_manual(values =  c("F" = "#d95f02", "NF" = "#1b9e77"), name = "Class", ) +  # Apply colors to Class
      scale_shape_manual(values = shapes, name = "City") +  # Apply shapes to City
      labs(
        title = paste("Scatterplot of", x_var, "vs", y_var),
        x = x_var,
        y = y_var
      ) +
      theme_minimal() +
      theme(
        legend.position = "top",
        legend.direction = "horizontal",
        legend.title = element_text(face = "bold"),
        legend.text = element_text(size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5)
      )
    
    print(plot)
  }
}


```

That shit above didn't work we will come back to it later, for now we continue

Class imbalance:

```{r}

library(ggplot2)

# Create a new data frame for plotting without modifying rawData
rawDataForPlot <- rawData

# Ensure Class is a factor with proper levels in the new data frame
rawDataForPlot$Class <- factor(rawDataForPlot$Class, levels = c("F", "NF"))

# Create a new data frame for the counts of each class
class_counts <- as.data.frame(table(rawDataForPlot$Class))
colnames(class_counts) <- c("Class", "Count")

# Create a bar plot of the Class variable with specified colors and count labels
ggplot(rawDataForPlot, aes(x = Class)) +
  geom_bar(aes(fill = Class), color = "black") +  # Fill bars based on Class
  scale_fill_manual(values = c("F" = "#d95f02", "NF" = "#1b9e77")) +  # Custom colors for F and NF
  labs(
    title = "Distribution of Classes",
    x = "Class",
    y = "Count"
  ) +
  geom_text(data = class_counts, aes(x = Class, y = Count, label = Count), vjust = -0.5) +  # Add count labels above bars
  theme_minimal() +
  theme(
    legend.position = "none",  # Remove the legend (optional)
    plot.title = element_text(face = "bold", hjust = 0.5)  # Center and bold title
  )


```
Classes don't look imbalanced 



# Data Pre-Processing
we will do a generic pre-processing here, all but normalizing the numeric values, this is because different ML models will benefit from different normalization methods:

```{r}
# Create a new data frame for preprocessing to avoid modifying rawData
rawDataForML <- rawData

# Step 1: Drop the 'Year' column
rawDataForML$Year <- NULL

# Step 2: Encode 'Class' and 'City'
# Encode 'Class' as -1 for 'F' and 1 for 'NF'
rawDataForML$Class <- ifelse(rawDataForML$Class == "F", -1, 1)

# Encode 'City' as -1 for 'Cordillera' and 1 for 'Hudson Bay'
rawDataForML$City <- ifelse(rawDataForML$City == "Cordillera", -1, 1)

# Show the first few rows of the preprocessed data
head(rawDataForML)

```



we will use the distributions to guide the normalization process for each model 
```{r}
library(ggplot2)

# Select numeric columns
numerical_columns <- names(rawDataForML)[sapply(rawDataForML, is.numeric)]

# Loop through each numeric variable and plot the distribution
for (col in numerical_columns) {
  # Create a histogram for each numeric column
  plot <- ggplot(rawDataForML, aes_string(x = col)) +
    geom_histogram(bins = 30, fill = "#d95f02", color = "black", alpha = 0.7) +
    labs(
      title = paste("Distribution of", col),
      x = col,
      y = "Frequency"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", hjust = 0.5)
    )
  
  # Print the plot
  print(plot)
}

```

# Model Development

## Logistic Regression



In this section, we will build a **Logistic Regression** model to predict whether a fire will occur based on environmental factors. The outcome we are predicting is categorical, either **"fire"** or **"no fire"**. 

### Why Logistic Regression?

Logistic regression is used for classification tasks, making it suitable for predicting categorical outcomes like "fire" or "no fire". Since the goal is to classify whether a fire is likely to occur, logistic regression is an appropriate choice. 

### Model Evaluation

We will assess the performance of our model using the **ROC curve** (Receiver Operating Characteristic curve) and the **AUC** (Area Under Curve). The ROC curve compares the true positive rate (sensitivity) with the false positive rate (1-specificity). A **higher ROC curve** indicates better performance, and a **higher AUC (closer to 1)** indicates a better model fit.

### Steps for Building the Model

We will first build the logistic regression model without normalizing the data and then compare the results after applying **Z-score normalization**. Normalization is essential in logistic regression because it standardizes features that have different units or scales. Without normalization, features with larger ranges can dominate the model and impact its accuracy.

### 1. Build Model Without Normalization

```r
# Split the data into training and testing sets (80% train, 20% test)
set.seed(123)
trainIndex <- createDataPartition(rawData$Classes, p = 0.8, list = FALSE)
trainData <- rawData[trainIndex, ]
testData <- rawData[-trainIndex, ]

# Fit the Logistic Regression model
logit_model <- glm(Classes ~ Temp + RH + Ws + Rain + FFMC + DMC + DC + ISI + BUI + FWI, 
                   data = trainData, 
                   family = binomial)

# Summary of the model
summary(logit_model)

# Predict on the test data
predictions <- predict(logit_model, testData, type = "response")
predicted_classes <- ifelse(predictions > 0.5, "fire", "not fire")

# Confusion Matrix
conf_matrix <- confusionMatrix(factor(predicted_classes), testData$Classes)
print(conf_matrix)

# Calculate ROC curve and AUC
roc_curve <- roc(testData$Classes, predictions)
plot(roc_curve, main = "ROC Curve - Logistic Regression")
auc(roc_curve)


## Linear Discriminant Analysis

## Quadratic Discriminant Analysis

## K-Nearest Neighbors Classifier

## Tree Based Methods

## Support Vector Machine

# ## Support Vector Machine (SVM) Model

In this section, we will build a Support Vector Machine (SVM) model to predict if a fire will occur 
 based on environmental factors (independent variables). SVM works by finding the hyperplane that best  separates the classes in the feature space. We will first train the SVM model using the raw data (without normalization) 
and evaluate it using the confusion matrix, ROC curve, and AUC. Afterward, we will normalize the data using Z-score normalization 
to see if it improves the model’s performance.

# ### SVM without Normalization

## Loading necessary libraries
if(!require(e1071)) install.packages("e1071", dependencies=TRUE)  # Install the e1071 package if not already installed
library(e1071)  # Load the e1071 package, which includes functions for SVM

## Split the data into training (80%) and testing (20%) sets
set.seed(123)  # Set seed for reproducibility
trainIndex <- createDataPartition(rawData$Classes, p = 0.8, list = FALSE)  # 80% training data
trainData <- rawData[trainIndex, ]  # Extract training dataset
testData <- rawData[-trainIndex, ]  # Extract testing dataset

## Train the SVM model using the 'Classes' as the dependent variable
svm_model <- svm(Classes ~ Temp + RH + Ws + Rain + FFMC + DMC + DC + ISI + BUI + FWI,
                 data = trainData,  # Use training data
                 kernel = "linear",  # Use linear kernel
                 type = "C-classification")  # Classification problem

## Make predictions using the test data
svm_predictions <- predict(svm_model, newdata = testData)

## Evaluate the performance using a confusion matrix
confusionMatrix(svm_predictions, testData$Classes)

## Generate the ROC curve and calculate the AUC (Area Under the Curve)
svm_roc <- roc(testData$Classes, as.numeric(svm_predictions), levels=rev(levels(testData$Classes)))
plot(svm_roc, main = "ROC Curve for SVM", col = "blue", lwd = 2)
auc(svm_roc)  # Display the AUC score

# The SVM model without normalization is now trained and evaluated.

# ### SVM with Normalization (Z-score)

# Normalization is important for models like SVM as it ensures all features are on the same scale.
# We'll use Z-score normalization to standardize the data before training the SVM model.

## Apply Z-score normalization to the numerical columns
numerical_columns <- c("Temp", "RH", "Ws", "Rain", "FFMC", "DMC", "DC", "ISI", "BUI", "FWI")

# Normalize the numerical columns using Z-score
rawData_normalized <- rawData
rawData_normalized[numerical_columns] <- scale(rawData[numerical_columns])

## Split the normalized data into training and testing sets
trainData_normalized <- rawData_normalized[trainIndex, ]
testData_normalized <- rawData_normalized[-trainIndex, ]

## Train the SVM model using normalized data
svm_model_normalized <- svm(Classes ~ Temp + RH + Ws + Rain + FFMC + DMC + DC + ISI + BUI + FWI,
                            data = trainData_normalized,  # Use normalized training data
                            kernel = "linear",  # Use linear kernel
                            type = "C-classification")  # Classification problem

## Make predictions using the normalized test data
svm_predictions_normalized <- predict(svm_model_normalized, newdata = testData_normalized)

## Evaluate the performance using a confusion matrix for normalized data
confusionMatrix(svm_predictions_normalized, testData_normalized$Classes)

## Generate the ROC curve and calculate the AUC (Area Under the Curve) for normalized data
svm_roc_normalized <- roc(testData_normalized$Classes, as.numeric(svm_predictions_normalized), levels=rev(levels(testData_normalized$Classes)))
plot(svm_roc_normalized, main = "ROC Curve for Normalized SVM", col = "red", lwd = 2)
auc(svm_roc_normalized)  # Display the AUC score

# The SVM model with normalization is now trained and evaluated.


## Analysis

# Results and Interpretation

# Conclusion
